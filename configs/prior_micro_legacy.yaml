model:
  target: "prior.diffusion_prior.LegacyDiffusionPrior"
  params:
    optimizer_config:
      target: "torch.optim.AdamW"
      params:
        lr: 2e-3
        betas: [0.9, 0.999]
        eps: 1e-08
        weight_decay: 0.01
    lr_scheduler_config: # 10000 warmup steps
      target: prior.optim.LambdaLinearScheduler
      params:
        warm_up_steps: [100]
        cycle_lengths: [10000000000000] # incredibly large number to prevent corner cases
        f_start: [1.e-10]
        f_max: [1.]
        f_min: [0.05]
    language_model_config:
      target: "dalle2_pytorch.dalle2_pytorch.OpenAIClipAdapter"
      params:
        name: "ViT-L/14"
    noise_scheduler_config:
      target: "prior.gaussian_diffusion.NoiseScheduler"
      params:
        beta_schedule: "cosine"
        timesteps: 1000
        loss_type: "l2"
    prior_transformer_config:
      target: "prior.prior_transformer.PriorTransformer"
      params:
        ctx_len: 77
        emb_dim: 768
        num_layers: 12
        num_heads: 12
        final_ln: true
        clip_dim: 768
        dropout: 0.00
trainer:
  train_data_urls: "data_url"
  val_data_urls: "data_url"
  epoch_length: null
  train_batch_size: 256
  valid_batch_size: 32
  wandb_project: "prior-testing"
  precision: "bf16-mixed"
  max_epochs: 1
  gradient_clip_val: 0.5
  accumulate_grad_batches: 1
  limit_val_batches: 1
  val_check_interval: 100
  enable_checkpointing: false
  checkpoint_dirpath: "checkpoints"
  checkpoint_save_top_k: 4
  checkpoint_monitor: "global_step"
  checkpoint_mode: "max"
  checkpoint_filename: "prior-b-32-{epoch:02d}-{global_step:06d}"
  checkpoint_train_time_interval_minutes: 60
