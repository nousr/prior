model:
  target: "prior.diffusion_prior.LegacyDiffusionPrior"
  params:
    optimizer_config:
      target: "torch.optim.AdamW"
      params:
        lr: 2e-4
        betas: [0.9, 0.999]
        eps: 1e-08
        weight_decay: 0.01
    lr_scheduler_config:
      target: prior.optim.LambdaLinearScheduler
      params:
        warm_up_steps: [1000]
        cycle_lengths: [10000000000000] # incredibly large number to prevent corner cases
        f_start: [1.e-10]
        f_max: [1.]
        f_min: [1.]
    language_model_config:
      target: "prior.adapter.OpenClipAdapter"
      params:
        path: "hf-hub:laion/CLIP-ViT-bigG-14-laion2B-39B-b160k"
    noise_scheduler_config:
      target: "prior.gaussian_diffusion.NoiseScheduler"
      params:
        beta_schedule: "cosine"
        timesteps: 1000
        loss_type: "l2"
    prior_transformer_config:
      target: "prior.prior_transformer.PriorTransformer"
      params:
        ctx_len: 77
        emb_dim: 1280
        num_layers: 20
        num_heads: 20
        final_ln: true
        clip_dim: 1280
        dropout: 0.05
trainer:
  train_data_urls: "pipe: aws s3 cp --quiet s3://stability-west/laion_coyo_dedupedv3/{00000..99998}.tar -"
  val_data_urls: "pipe: aws s3 cp --quiet s3://stability-west/laion_coyo_dedupedv3/99999.tar -"
  epoch_length: 2037612045
  train_batch_size: 200
  valid_batch_size: 32
  wandb_project: "prior-testing"
  precision: "bf16-mixed"
  max_epochs: 1
  gradient_clip_val: 0.5
  accumulate_grad_batches: 1
  limit_val_batches: 1
  val_check_interval: 100
  enable_checkpointing: true
  checkpoint_dirpath: "checkpoints"
  checkpoint_save_top_k: 4
  checkpoint_monitor: "global_step"
  checkpoint_mode: "max"
  checkpoint_filename: "prior-big-g-{epoch:02d}-{global_step:06d}"
  checkpoint_train_time_interval_minutes: 60