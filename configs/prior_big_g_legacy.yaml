model:
  target: "prior.diffusion_prior.LegacyDiffusionPrior"
  params:
    image_embedding_stats_path: "/admin/home-zion/prior/prior/stats/openclip_bigG_image_stats.pt"
    optimizer_config:
      target: "torch.optim.AdamW"
      params:
        lr: 1e-4
        betas: [0.9, 0.999]
        eps: 1.0e-08
        weight_decay: 0.01
    lr_scheduler_config:
      target: "prior.optim.LambdaLinearScheduler"
      params:
        warm_up_steps: [1000]
        cycle_lengths: [10000000000000] # incredibly large number to prevent corner cases
        f_start: [1.e-10]
        f_max: [1.]
        f_min: [1.]
    language_model_config:
      target: "prior.adapter.OpenClipAdapter"
      params:
        path: "hf-hub:laion/CLIP-ViT-bigG-14-laion2B-39B-b160k"
    noise_scheduler_config:
      target: "prior.gaussian_diffusion.NoiseScheduler"
      params:
        beta_schedule: "cubic"
        timesteps: 1000
        loss_type: "l2"
    prior_transformer_config:
      target: "prior.prior_transformer.PriorTransformer"
      params:
        ctx_len: 77
        emb_dim: 1280
        num_layers: 20
        num_heads: 20
        final_ln: true
        clip_dim: 1280
        dropout: 0.05
        causal: false
trainer:
  ckpt_path: null
  train_data_urls: "data_url"
  val_data_urls: "data_url"
  epoch_length: 2037612045
  train_batch_size: 200
  valid_batch_size: 32
  wandb_project: "prior-testing"
  precision: "bf16-mixed"
  max_epochs: 2
  gradient_clip_val: 0.5
  accumulate_grad_batches: 1
  limit_val_batches: 1
  val_check_interval: 200
  enable_checkpointing: true
  checkpoint_dirpath: "checkpoints-cubic"
  checkpoint_save_top_k: 4
  checkpoint_monitor: "step"
  checkpoint_mode: "max"
  checkpoint_filename: "prior-big-g-{step:06d}"
  checkpoint_train_time_interval_minutes: 60
